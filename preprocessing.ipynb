{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dython.nominal import associations\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Ignore future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS_MAPPING_FN = \"./data/IDS_mapping.csv\"\n",
    "DIABETIC_FN = \"./data/diabetic_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "mapping = pd.read_csv(IDS_MAPPING_FN, header=None)\n",
    "df = pd.read_csv(DIABETIC_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIC_COLUMNS = df.columns[24:46].tolist()\n",
    "MEDIC_COLUMNS_TAKE = [\"take_\" + med for med in MEDIC_COLUMNS]\n",
    "PREVIOUS_HOSPITAL_ENCOUNTERS = [\"number_outpatient\", \"number_inpatient\", \"number_emergency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of code-value mappings of `admission_type_id`, `discharge_disposition_id`, and `admission_source_id` using the mapping provided in the data-folder, and map integer values to string values for readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_type_dict = {}\n",
    "discharge_disposition_dict = {}\n",
    "admission_source_dict = {}\n",
    "\n",
    "list1 = []\n",
    "for i, j in zip(mapping[0].values, mapping[1].values):\n",
    "  if len(str(i))>3:\n",
    "    feature_name = i\n",
    "  elif len(str(i))!=3:\n",
    "    if feature_name == 'admission_type_id':\n",
    "      admission_type_dict[int(i)] = j\n",
    "    elif feature_name == 'discharge_disposition_id':\n",
    "      discharge_disposition_dict[int(i)] = j\n",
    "    elif feature_name == 'admission_source_id':\n",
    "      admission_source_dict[int(i)] = j\n",
    "\n",
    "\n",
    "df['admission_type'] = df['admission_type_id'].apply(lambda x: admission_type_dict[x])\n",
    "df['discharge_disposition'] = df['discharge_disposition_id'].apply(lambda x: discharge_disposition_dict[x])\n",
    "df['admission_source'] = df['admission_source_id'].apply(lambda x: admission_source_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique encounters: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some patients have many encounters (up to 40).\n",
    "\n",
    "We only keep the first observation for each unique patient to treat them as i.i.d random variables.\n",
    "\n",
    "We filter to only keep observations with `admission_type` $\\in$ [Emergency, Urgent, Elective]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(\"patient_nbr\").first(skipna=False).reset_index()\n",
    "df = df[df['admission_type'].isin(['Emergency', 'Urgent', 'Elective'])]\n",
    "print(f\"Number of unique encounters after only keeping first encounter for each patient and filtering by admission_type : {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Diabetic Information\n",
    "\n",
    "To select features as predictor variables we are only interested in certain information about the patients, but not the exact values. Thus we aggregate the information of sets of columns into new columns:\n",
    "\n",
    "\n",
    "* Blood Glucose Tests\n",
    "\n",
    "    * `max_glu_serum_flag`: Whether a max glucose serum test was done at the hospital : `max_glu_serum`\n",
    "\n",
    "\n",
    "    * `A1C_flag`: Whether an AC1 test was performed to monitor blood glucose levels : `AC1result`\n",
    "\n",
    "    \n",
    "* Diabetic Medication Information: \n",
    "\n",
    "    * `change_dosage`: Whether there was any change (\"Up\" or \"Down\") in the diabetic prescription dosages as a result of the hospital visit.\n",
    "        * *The column `change` indicates if there was a change in diabetic medications (either dosage or generic name). So if `change` is marked as changed but `change_dosage` is none, then there must have been a change in the generic name, i.e. the chemical name of a medicine.*\n",
    "\n",
    "\n",
    "    * `change_medicine`: Whether there was prescribed any new diabetic medication i.e. a change in the medicament as a result of the hospital visit.\n",
    "\n",
    "\n",
    "    * `num_diabetic_prescriptions`: How many diabetic prescriptions the patient ongoingly had at the time of hospital visit - a count of entries that are $\\in \\{\\text{\"Steady\"}, \\text{\"Up\"}, \\text{\"Down\"}\\}$ in `MEDIC_COLUMNS`\n",
    "\n",
    "    * `take_<medicine_name>`: Whether the patient takes the <medicine_name> \n",
    "\n",
    "\n",
    "* Admitted in the hospital within the previous year\n",
    "\n",
    "    * `prev_year_hospital`: Whether the patient had any admissions in the hospital during the past year. \n",
    "\n",
    "* Health Insurance / Coverage\n",
    "\n",
    "    * `blue_cross`: Patient has private insurance\n",
    "    * `medicaid`: Patient has medicaid\n",
    "    * `medicare`: Patient has medicare\n",
    "    * `self_payed`: Patient payed up front\n",
    "  \n",
    "* `discharge_disposition_id`: is aggregated into `home`, `transfer`, `unknown`, and `other`\n",
    "\n",
    "* `Age` is aggregated into `[0-30)`, `[30-60)`, and `[60-100)`\n",
    "\n",
    "* Readmitted\n",
    "    * `readmitted_flag`: Whether the patient was readmitted or not based on the columns `readmitted`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Blood glucose Tests ####\n",
    "df['max_glu_serum_flag'] = df['max_glu_serum'].notnull().astype(int) # Max Glucose Serum test flag\n",
    "df['A1C_flag'] = df['A1Cresult'].notnull().astype(int) # AC1 test flag\n",
    "\n",
    "#### Diabetic Medication Information ####\n",
    "# Change in dosage if any of the diabetics prescriptions has entries \"Up\" or \"Down\"\n",
    "df['change_dosage'] = df[MEDIC_COLUMNS].isin(['Up', 'Down']).any(axis=1).astype(int)\n",
    "# Check if each entry is in the set ['Up', 'Down', 'Steady'] and sum all True entries for each row\n",
    "df['num_diabetic_prescriptions'] = df[MEDIC_COLUMNS].apply(lambda col: np.isin(col, ['Up', 'Down', 'Steady'])).sum(axis=1).astype(int)\n",
    "# Change in medicine is assumed to be the case when original change column = 1 but change in dosage = 0\n",
    "df['change_medicine'] = np.where((df['change'] == 'Ch') & (df['change_dosage'] == 0), 1, 0)\n",
    "\n",
    "# Whether the patient takes any of the medications\n",
    "for med in MEDIC_COLUMNS:\n",
    "    name = \"take_\" + med\n",
    "    df[name] = df[med].isin([\"Down\", \"Steady\", \"Up\"])\n",
    "\n",
    "#### Hospital Encounters during preceding year ####\n",
    "df['prev_year_hospital'] = (df[PREVIOUS_HOSPITAL_ENCOUNTERS] > 0).any(axis=1).astype(int)\n",
    "\n",
    "#### Insurance Billing ####\n",
    "df['blue_cross'] = np.where(df[\"payer_code\"]==\"BC\", 1, 0)\n",
    "df['medicare'] = np.where(df[\"payer_code\"]==\"MC\", 1, 0)\n",
    "df['medicaid'] = np.where(df[\"payer_code\"]==\"MD\", 1, 0)\n",
    "df['self_pay'] = np.where(df[\"payer_code\"]==\"SP\", 1, 0)\n",
    "\n",
    "#### Discharge disposition id aggregated ####\n",
    "df['discharge_disposition_id'] = df['discharge_disposition_id'].replace([1, 6, 8, 13], \"home\")\n",
    "df['discharge_disposition_id'] = df['discharge_disposition_id'].replace([2, 3, 4, 5, 9, 10, 14, 15, 16, 17, 22, 23, 24, 27, 28, 29], \"transfer\")\n",
    "df['discharge_disposition_id'] = df['discharge_disposition_id'].replace([18, 25, 26], \"unknown\")\n",
    "df['discharge_disposition_id'] = df['discharge_disposition_id'].replace([7, 11, 12, 19, 20, 21], \"other\")\n",
    "\n",
    "#### Readmitted ####\n",
    "# y_i\n",
    "df['readmitted_flag'] = np.where(df['readmitted']=='NO', 0, 1) # Readmitted flag\n",
    "\n",
    "#### Age aggregaed ####\n",
    "df['age'] = df['age'].replace(['[0-10)', '[10-20)','[20-30)'], '[0-30)')\n",
    "df['age'] = df['age'].replace(['[30-40)', '[40-50)','[50-60)'], '[30-60)')\n",
    "df['age'] = df['age'].replace(['[60-70)', '[70-80)','[80-90)', '[90-100)'], '[60-100)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_NUM = [\"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", 'num_diabetic_prescriptions', \n",
    "                \"num_medications\", 'prev_year_hospital', \"number_diagnoses\"]\n",
    "\n",
    "FEATURES_BIN = ['max_glu_serum_flag', 'A1C_flag', 'change_dosage', 'change_medicine', \"blue_cross\", \"medicaid\", \"medicare\",\"self_pay\"]\n",
    "\n",
    "FEATURES_CAT = ['discharge_disposition_id', \"admission_type_id\"]\n",
    "\n",
    "FEATURES = FEATURES_NUM + FEATURES_BIN + FEATURES_CAT + MEDIC_COLUMNS_TAKE\n",
    "\n",
    "PROTECTED_FEATURES = ['age','race', 'gender']\n",
    "PATIENTS = [\"patient_nbr\"]  \n",
    "TARGET = [\"readmitted_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[PATIENTS + FEATURES + PROTECTED_FEATURES + TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot = pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=FEATURES_CAT, drop_first=False) # dropf irst?\n",
    "df_one_hot = df_one_hot.drop([\"discharge_disposition_id_unknown\"], axis = 1)\n",
    "FEATURES_ONE_HOT = df_one_hot.drop(PATIENTS+PROTECTED_FEATURES+TARGET, axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersample majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_major = df_one_hot[df_one_hot['readmitted_flag'] == 0]\n",
    "df_minor = df_one_hot[df_one_hot['readmitted_flag'] == 1]\n",
    "\n",
    "df_downsamples = resample(df_major, replace=False, n_samples=len(df_minor), random_state=42)  \n",
    "\n",
    "df_one_hot = pd.concat([df_downsamples, df_minor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_one_hot[FEATURES_ONE_HOT]\n",
    "y = df_one_hot[TARGET].to_numpy().reshape(-1)\n",
    "protected_features = df_one_hot[PROTECTED_FEATURES]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(X, y, protected_features, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[FEATURES_NUM])\n",
    "X_train[FEATURES_NUM] = scaler.transform(X_train[FEATURES_NUM])\n",
    "X_test[FEATURES_NUM] = scaler.transform(X_test[FEATURES_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = associations(pd.concat([df_one_hot[FEATURES_ONE_HOT], df_one_hot[TARGET]], axis=1), figsize=(18, 15))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='vlag', fmt=\".2f\")\n",
    "# plt.title(\"Correlation Heatmap of Features with Target\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grid search and Cross Validation we found an inverse regularisation strength of $0.1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=5000, penalty='l2', C=0.1, tol=1e-4, solver = \"saga\")\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "y_pred_proba_0 = logistic_model.predict_proba(X_test)[:,0]\n",
    "y_pred_proba_1 = logistic_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"False Positive Rate:\", fpr[1])\n",
    "print(\"True Positive Rate:\", tpr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protected_features_predicions =  pd.concat([group_test, pd.DataFrame({'y_test': y_test, 'y_pred': y_pred, \"y_proba_0\":y_pred_proba_0, \"y_proba_1\":y_pred_proba_1}, index=X_test.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(group_name, df_protected_features_predicions=df_protected_features_predicions):\n",
    "    group_names, accuracies, tprs, fprs = [], [], [], []\n",
    "\n",
    "    # Group by 'age', 'race', and 'gender' columns\n",
    "    groups = df_protected_features_predicions.groupby(group_name)\n",
    "\n",
    "    for group, group_df in groups:\n",
    "        group_names.append(group)\n",
    "        y_true = group_df['y_test']\n",
    "        y_pred = group_df['y_pred']\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        tprs.append(tpr[1])\n",
    "        fprs.append(fpr[1])\n",
    "        \n",
    "    return group_names, accuracies, tprs, fprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names, accuracies, tprs, fprs = get_metrics('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_names, tprs, fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_proba_protected_groups(group, df_protected_features_predicions=df_protected_features_predicions):\n",
    "    proba_preds, true, protected_groups = [], [], []\n",
    "\n",
    "    for group, group_df in df_protected_features_predicions.groupby(group):\n",
    "        y_true = group_df['y_test']\n",
    "        y_proba_0 = group_df[\"y_proba_0\"]\n",
    "        y_proba_1 = group_df[\"y_proba_1\"]\n",
    "        protected_groups.append(group)\n",
    "        proba_preds.append(np.column_stack((y_proba_0,y_proba_1)))\n",
    "        true.append(y_true)\n",
    "        \n",
    "    return proba_preds, true, protected_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_tpr_fpr(y_real, y_pred):\n",
    "    # Calculates the confusion matrix and recover each element\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    # Calculates tpr and fpr\n",
    "    tpr =  TP/(TP + FN) # sensitivity - true positive rate\n",
    "    fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate\n",
    "    \n",
    "    return tpr, fpr\n",
    "\n",
    "def get_n_roc_coordinates(y_real, y_proba, n = 100):\n",
    "    tpr_list = [0]\n",
    "    fpr_list = [0]\n",
    "    for i in range(n):\n",
    "        threshold = i/n\n",
    "        y_pred = y_proba[:, 1] > threshold\n",
    "        tpr, fpr = calculate_tpr_fpr(y_real, y_pred)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    return tpr_list, fpr_list\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, ax, group, scatter = False):\n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax=ax,label=group)\n",
    "    sns.lineplot(x = [0, 1], y = [0, 1], color = 'black', ax=ax, linestyle='--')\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "\n",
    "_, axes = plt.subplots(1,3,figsize = (12, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for j, pf in enumerate(PROTECTED_FEATURES):\n",
    "    proba_preds, true, protected_groups = y_proba_protected_groups(pf)\n",
    "    for i in range(len(protected_groups)):\n",
    "        tpr, fpr = get_n_roc_coordinates(true[i], proba_preds[i])\n",
    "        plot_roc_curve(tpr, fpr, ax=axes[j], group=protected_groups[i],scatter = False)\n",
    "\n",
    "axes[1].set_title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true[1], proba_preds[1][:,1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "roc_display.figure_.set_size_inches(5,5)\n",
    "plt.plot([0, 1], [0, 1], color = 'g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto from group example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "result_metrics = {}\n",
    "for l_value in tqdm(lambda_values):\n",
    "    scaler = debias_data(protected_cols, nonprotected_cols, lambda_=l_value)\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    model = LogisticRegression\n",
    "    model_params = {\"max_iter\":500, \"random_state\":1}\n",
    "\n",
    "    preds = test_model(model, X, y, kfold,scaler=scaler, model_params=model_params)\n",
    "    predictions[f\"{l_value:.2f}\"] = preds\n",
    "    \n",
    "    acc_black = accuracy_score(data_y.HighCrime.values[IsBlack], preds[IsBlack])\n",
    "    acc_non_black = accuracy_score(data_y.HighCrime.values[~IsBlack], preds[~IsBlack])\n",
    "    recall_black, fpr_black = equalized_odds(data_y.HighCrime.values[IsBlack], preds[IsBlack])\n",
    "    recall_non_black, fpr_non_black = equalized_odds(data_y.HighCrime.values[~IsBlack], preds[~IsBlack])\n",
    "    num_high_black = np.sum(data_y.HighCrime.values[IsBlack]), np.sum(preds[IsBlack])\n",
    "    num_highn_non_black = np.sum(data_y.HighCrime.values[~IsBlack]), np.sum(preds[~IsBlack])\n",
    "    result_metrics[f\"{l_value:.2f}\"] = {\"acc_black\":acc_black,\n",
    "                                        \"acc_non_black\": acc_non_black,\n",
    "                                        \"recall_black\":recall_black, \n",
    "                                        \"recall_non_black\":recall_non_black,\n",
    "                                         \"fpr_black\": fpr_black,\n",
    "                                         \"fpr_non_black\": fpr_non_black,\n",
    "                                        \"num_high_black\":num_high_black,\n",
    "                                       \"num_highn_non_black\":num_highn_non_black}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makro_acc = []\n",
    "equal_odds_mse = []\n",
    "for key, res in result_metrics.items():\n",
    "    m_acc = (res[\"acc_black\"] + res[\"acc_non_black\"])/2\n",
    "    se_recall = (res[\"recall_black\"] - res[\"recall_non_black\"])**2\n",
    "    se_fpr = (res[\"fpr_black\"] - res[\"fpr_non_black\"])**2\n",
    "    mse_total = (se_recall + se_fpr)/2\n",
    "    \n",
    "    makro_acc.append(m_acc)\n",
    "    equal_odds_mse.append(mse_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_ax = [0, 7, 12, 18, 20, 23, 25, 29]\n",
    "y_upper = [x + 10 for x in equal_odds_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "ax.plot(makro_acc, equal_odds_mse, color = \"black\", linewidth = 0.5)\n",
    "\n",
    "for j in np.arange(0,len(lambda_values),1):\n",
    "    if j in annotate_ax:\n",
    "        color = 'turquoise'\n",
    "    else: \n",
    "        color = 'seagreen'\n",
    "    ax.scatter(makro_acc[j], equal_odds_mse[j], color = color)\n",
    "\n",
    "for i in annotate_ax:\n",
    "    ax.annotate(\"$\\lambda$ = \"+str(np.round(lambda_values[i],2)), (makro_acc[i] + 0.01, equal_odds_mse[i]), fontsize = 12)\n",
    "\n",
    "ax.fill_between(makro_acc, equal_odds_mse, y_upper, color = \"seagreen\", alpha = 0.1)\n",
    "\n",
    "ax.set_xlim(right = 0.86)\n",
    "ax.set_ylim(bottom = -0.005, top = 0.08)\n",
    "\n",
    "plt.title(\"Logreg pareto front: fairness vs. accuracy\", fontsize = 20)\n",
    "plt.xlabel(r\"Macro accuracy\", fontsize = 14)\n",
    "plt.ylabel(\"Mean squerred difference in equalized odds\", fontsize = 14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
